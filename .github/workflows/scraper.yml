name: Run Scraper

on:
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Choose a label for this run (appears in workflow summary).'
        required: false
        default: 'manual'
      profiles_count:
        description: 'How many profiles to scrape? (1=10, 2=50, 3=100)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '5'
      comment:
        description: 'Optional note explaining why the workflow was triggered.'
        required: false
  schedule:
    - cron: '0 */2 * * *'

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Setup Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Add ChromeDriver to PATH
        run: |
          echo "CHROMEDRIVER_PATH=$(which chromedriver)" >> $GITHUB_ENV

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Restore service account credentials
        run: |
          echo "$DD_CREDENTIALS_JSON" > credentials.json
        env:
          DD_CREDENTIALS_JSON: ${{ secrets.DD_CREDENTIALS_JSON }}

      - name: Run scraper
        env:
          DD_LOGIN_EMAIL: ${{ secrets.DD_LOGIN_EMAIL }}
          DD_LOGIN_PASS: ${{ secrets.DD_LOGIN_PASS }}
          DD_SHEET_ID: ${{ secrets.DD_SHEET_ID }}
          COOKIE_FILE: ${{ secrets.COOKIE_FILE }}
        run: python Scraper.py
